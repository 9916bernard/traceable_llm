"""
ë‹¤ì–‘í•œ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ìŠ¤í¬ë¦½íŠ¸

ì˜ì–´ ì¤‘ì‹¬ ë©€í‹°ë­ê·€ì§€ í”„ë¡¬í”„íŠ¸ í•„í„°ë§ì„ ìœ„í•œ ìµœì  ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤.
"""

import torch
import torch.nn.functional as F
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments, 
    Trainer
)
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
import pandas as pd
import numpy as np
import json
from pathlib import Path
import time
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

class ModelComparator:
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self, data_dir: str = "datasets"):
        self.data_dir = Path(data_dir)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ë“¤
        self.models_to_test = [
            "xlm-roberta-base",           # ë©€í‹°ë­ê·€ì§€ RoBERTa
            "bert-base-multilingual-cased",  # ë©€í‹°ë­ê·€ì§€ BERT
            "distilbert-base-multilingual-cased",  # ê²½ëŸ‰í™” BERT
            "roberta-base",               # ì˜ì–´ RoBERTa
            "bert-base-uncased"           # ì˜ì–´ BERT
        ]
        
        # ë ˆì´ë¸” ë§¤í•‘
        self.label2id = {
            'APPROPRIATE': 0,
            'JAILBREAK': 1,
            'HARMFUL': 2,
            'ADULT': 3,
            'MEANINGLESS': 4
        }
        self.id2label = {v: k for k, v in self.label2id.items()}
    
    def load_test_data(self) -> Tuple[List[str], List[str]]:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ"""
        test_path = self.data_dir / "test.json"
        
        if not test_path.exists():
            raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_path}")
        
        with open(test_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        texts = [item['text'] for item in data]
        labels = [item['label'] for item in data]
        
        return texts, labels
    
    def test_model_performance(self, model_name: str, test_texts: List[str], test_labels: List[str]) -> Dict:
        """ë‹¨ì¼ ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"\n{model_name} ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        try:
            # ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                num_labels=5,
                problem_type="single_label_classification"
            )
            
            model.to(self.device)
            model.eval()
            
            # ëª¨ë¸ ì„¤ì •
            model.config.id2label = self.id2label
            model.config.label2id = self.label2id
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            predictions = []
            inference_times = []
            
            with torch.no_grad():
                for text in test_texts:
                    start_time = time.time()
                    
                    # í† í¬ë‚˜ì´ì§•
                    inputs = tokenizer(
                        text,
                        truncation=True,
                        padding=True,
                        max_length=512,
                        return_tensors='pt'
                    )
                    
                    # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                    inputs = {k: v.to(self.device) for k, v in inputs.items()}
                    
                    # ì˜ˆì¸¡
                    outputs = model(**inputs)
                    predictions_tensor = F.softmax(outputs.logits, dim=-1)
                    predicted_class_id = torch.argmax(predictions_tensor, dim=-1).item()
                    
                    end_time = time.time()
                    inference_times.append(end_time - start_time)
                    
                    predictions.append(predicted_class_id)
            
            # ë ˆì´ë¸” ì¸ì½”ë”©
            true_labels = [self.label2id[label] for label in test_labels]
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
            accuracy = accuracy_score(true_labels, predictions)
            precision, recall, f1, _ = precision_recall_fscore_support(
                true_labels, predictions, average='weighted'
            )
            
            # í‰ê·  ì¶”ë¡  ì‹œê°„
            avg_inference_time = np.mean(inference_times)
            
            # ëª¨ë¸ í¬ê¸° (ëŒ€ëµì )
            model_size = sum(p.numel() for p in model.parameters()) / 1e6  # MB
            
            return {
                'model_name': model_name,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'avg_inference_time': avg_inference_time,
                'model_size_mb': model_size,
                'success': True
            }
            
        except Exception as e:
            print(f"ëª¨ë¸ {model_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            return {
                'model_name': model_name,
                'accuracy': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'f1_score': 0.0,
                'avg_inference_time': 0.0,
                'model_size_mb': 0.0,
                'success': False,
                'error': str(e)
            }
    
    def compare_all_models(self) -> pd.DataFrame:
        """ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        test_texts, test_labels = self.load_test_data()
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_texts)}ê°œ")
        
        # ê° ëª¨ë¸ í…ŒìŠ¤íŠ¸
        results = []
        
        for model_name in self.models_to_test:
            result = self.test_model_performance(model_name, test_texts, test_labels)
            results.append(result)
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(results)
        
        # ì„±ê³µí•œ ëª¨ë¸ë§Œ í•„í„°ë§
        successful_models = df[df['success'] == True].copy()
        
        # ì„±ëŠ¥ ìˆœìœ¼ë¡œ ì •ë ¬
        successful_models = successful_models.sort_values('f1_score', ascending=False)
        
        return successful_models
    
    def print_comparison_results(self, results_df: pd.DataFrame):
        """ë¹„êµ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
        print("="*80)
        
        for _, row in results_df.iterrows():
            print(f"\nëª¨ë¸: {row['model_name']}")
            print(f"  ì •í™•ë„: {row['accuracy']:.4f}")
            print(f"  F1 ì ìˆ˜: {row['f1_score']:.4f}")
            print(f"  ì •ë°€ë„: {row['precision']:.4f}")
            print(f"  ì¬í˜„ìœ¨: {row['recall']:.4f}")
            print(f"  í‰ê·  ì¶”ë¡  ì‹œê°„: {row['avg_inference_time']:.4f}ì´ˆ")
            print(f"  ëª¨ë¸ í¬ê¸°: {row['model_size_mb']:.1f}MB")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶”ì²œ
        if not results_df.empty:
            best_model = results_df.iloc[0]
            print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['model_name']}")
            print(f"   F1 ì ìˆ˜: {best_model['f1_score']:.4f}")
            print(f"   ì •í™•ë„: {best_model['accuracy']:.4f}")
    
    def save_comparison_results(self, results_df: pd.DataFrame, filename: str = "model_comparison_results.json"):
        """ë¹„êµ ê²°ê³¼ ì €ì¥"""
        output_path = self.data_dir / filename
        
        # DataFrameì„ JSONìœ¼ë¡œ ë³€í™˜
        results_dict = results_df.to_dict('records')
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
        
        print(f"ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_path}")
        return output_path

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    comparator = ModelComparator()
    
    # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
    results_df = comparator.compare_all_models()
    
    # ê²°ê³¼ ì¶œë ¥
    comparator.print_comparison_results(results_df)
    
    # ê²°ê³¼ ì €ì¥
    comparator.save_comparison_results(results_df)
    
    print("\nëª¨ë¸ ë¹„êµ ì™„ë£Œ!")

if __name__ == "__main__":
    main()

